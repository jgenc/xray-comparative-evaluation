[[1;31mDEBUG[0m] Model: MMDataParallel(
  (module): CoDETR(
    (backbone): ViT(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))
      )
      (rope_win): VisionRotaryEmbeddingFast()
      (rope_glb): None
      (blocks): ModuleList(
        (0): Block(
          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q_proj): Linear(in_features=1024, out_features=1024, bias=False)
            (k_proj): Linear(in_features=1024, out_features=1024, bias=False)
            (v_proj): Linear(in_features=1024, out_features=1024, bias=False)
            (rope): VisionRotaryEmbeddingFast()
            (proj): Linear(in_features=1024, out_features=1024, bias=True)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (mlp): SwiGLU(
            (w1): Linear(in_features=1024, out_features=2730, bias=True)
            (w2): Linear(in_features=1024, out_features=2730, bias=True)
            (act): SiLU()
            (ffn_ln): LayerNorm((2730,), eps=1e-06, elementwise_affine=True)
            (w3): Linear(in_features=2730, out_features=1024, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): Block(
          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q_proj): Linear(in_features=1024, out_features=1024, bias=False)
            (k_proj): Linear(in_features=1024, out_features=1024, bias=False)
            (v_proj): Linear(in_features=1024, out_features=1024, bias=False)
            (rope): VisionRotaryEmbeddingFast()
            (proj): Linear(in_features=1024, out_features=1024, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.013)
          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (mlp): SwiGLU(
            (w1): Linear(in_features=1024, out_features=2730, bias=True)
            (w2): Linear(in_features=1024, out_features=2730, bias=True)
            (act): SiLU()
            (ffn_ln): LayerNorm((2730,), eps=1e-06, elementwise_affine=True)
            (w3): Linear(in_features=2730, out_features=1024, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): Block(
          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q_proj): Linear(in_features=1024, out_features=1024, bias=False)
            (k_proj): Linear(in_features=1024, out_features=1024, bias=False)
            (v_proj): Linear(in_features=1024, out_features=1024, bias=False)
            (proj): Linear(in_features=1024, out_features=1024, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.026)
          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (mlp): SwiGLU(
            (w1): Linear(in_features=1024, out_features=2730, bias=True)
            (w2): Linear(in_features=1024, out_features=2730, bias=True)
            (act): SiLU()
            (ffn_ln): LayerNorm((2730,), eps=1e-06, elementwise_affine=True)
            (w3): Linear(in_features=2730, out_features=1024, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): Block(
          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q_proj): Linear(in_features=1024, out_features=1024, bias=False)
            (k_proj): Linear(in_features=1024, out_features=1024, bias=False)
            (v_proj): Linear(in_features=1024, out_features=1024, bias=False)
            (rope): VisionRotaryEmbeddingFast()
            (proj): Linear(in_features=1024, out_features=1024, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.039)
          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (mlp): SwiGLU(
            (w1): Linear(in_features=1024, out_features=2730, bias=True)
            (w2): Linear(in_features=1024, out_features=2730, bias=True)
            (act): SiLU()
            (ffn_ln): LayerNorm((2730,), eps=1e-06, elementwise_affine=True)
            (w3): Linear(in_features=2730, out_features=1024, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): Block(
          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q_proj): Linear(in_features=1024, out_features=1024, bias=False)
            (k_proj): Linear(in_features=1024, out_features=1024, bias=False)
            (v_proj): Linear(in_features=1024, out_features=1024, bias=False)
            (rope): VisionRotaryEmbeddingFast()
            (proj): Linear(in_features=1024, out_features=1024, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.052)
          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (mlp): SwiGLU(
            (w1): Linear(in_features=1024, out_features=2730, bias=True)
            (w2): Linear(in_features=1024, out_features=2730, bias=True)
            (act): SiLU()
            (ffn_ln): LayerNorm((2730,), eps=1e-06, elementwise_affine=True)
            (w3): Linear(in_features=2730, out_features=1024, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): Block(
          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q_proj): Linear(in_features=1024, out_features=1024, bias=False)
            (k_proj): Linear(in_features=1024, out_features=1024, bias=False)
            (v_proj): Linear(in_features=1024, out_features=1024, bias=False)
            (proj): Linear(in_features=1024, out_features=1024, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.065)
          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (mlp): SwiGLU(
            (w1): Linear(in_features=1024, out_features=2730, bias=True)
            (w2): Linear(in_features=1024, out_features=2730, bias=True)
            (act): SiLU()
            (ffn_ln): LayerNorm((2730,), eps=1e-06, elementwise_affine=True)
            (w3): Linear(in_features=2730, out_features=1024, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (6): Block(
          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q_proj): Linear(in_features=1024, out_features=1024, bias=False)
            (k_proj): Linear(in_features=1024, out_features=1024, bias=False)
            (v_proj): Linear(in_features=1024, out_features=1024, bias=False)
            (rope): VisionRotaryEmbeddingFast()
            (proj): Linear(in_features=1024, out_features=1024, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.078)
          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (mlp): SwiGLU(
            (w1): Linear(in_features=1024, out_features=2730, bias=True)
            (w2): Linear(in_features=1024, out_features=2730, bias=True)
            (act): SiLU()
            (ffn_ln): LayerNorm((2730,), eps=1e-06, elementwise_affine=True)
            (w3): Linear(in_features=2730, out_features=1024, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (7): Block(
          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q_proj): Linear(in_features=1024, out_features=1024, bias=False)
            (k_proj): Linear(in_features=1024, out_features=1024, bias=False)
            (v_proj): Linear(in_features=1024, out_features=1024, bias=False)
            (rope): VisionRotaryEmbeddingFast()
            (proj): Linear(in_features=1024, out_features=1024, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.091)
          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (mlp): SwiGLU(
            (w1): Linear(in_features=1024, out_features=2730, bias=True)
            (w2): Linear(in_features=1024, out_features=2730, bias=True)
            (act): SiLU()
            (ffn_ln): LayerNorm((2730,), eps=1e-06, elementwise_affine=True)
            (w3): Linear(in_features=2730, out_features=1024, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (8): Block(
          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q_proj): Linear(in_features=1024, out_features=1024, bias=False)
            (k_proj): Linear(in_features=1024, out_features=1024, bias=False)
            (v_proj): Linear(in_features=1024, out_features=1024, bias=False)
            (proj): Linear(in_features=1024, out_features=1024, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.104)
          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (mlp): SwiGLU(
            (w1): Linear(in_features=1024, out_features=2730, bias=True)
            (w2): Linear(in_features=1024, out_features=2730, bias=True)
            (act): SiLU()
            (ffn_ln): LayerNorm((2730,), eps=1e-06, elementwise_affine=True)
            (w3): Linear(in_features=2730, out_features=1024, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (9): Block(
          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q_proj): Linear(in_features=1024, out_features=1024, bias=False)
            (k_proj): Linear(in_features=1024, out_features=1024, bias=False)
            (v_proj): Linear(in_features=1024, out_features=1024, bias=False)
            (rope): VisionRotaryEmbeddingFast()
            (proj): Linear(in_features=1024, out_features=1024, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.117)
          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (mlp): SwiGLU(
            (w1): Linear(in_features=1024, out_features=2730, bias=True)
            (w2): Linear(in_features=1024, out_features=2730, bias=True)
            (act): SiLU()
            (ffn_ln): LayerNorm((2730,), eps=1e-06, elementwise_affine=True)
            (w3): Linear(in_features=2730, out_features=1024, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (10): Block(
          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q_proj): Linear(in_features=1024, out_features=1024, bias=False)
            (k_proj): Linear(in_features=1024, out_features=1024, bias=False)
            (v_proj): Linear(in_features=1024, out_features=1024, bias=False)
            (rope): VisionRotaryEmbeddingFast()
            (proj): Linear(in_features=1024, out_features=1024, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.130)
          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (mlp): SwiGLU(
            (w1): Linear(in_features=1024, out_features=2730, bias=True)
            (w2): Linear(in_features=1024, out_features=2730, bias=True)
            (act): SiLU()
            (ffn_ln): LayerNorm((2730,), eps=1e-06, elementwise_affine=True)
            (w3): Linear(in_features=2730, out_features=1024, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (11): Block(
          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q_proj): Linear(in_features=1024, out_features=1024, bias=False)
            (k_proj): Linear(in_features=1024, out_features=1024, bias=False)
            (v_proj): Linear(in_features=1024, out_features=1024, bias=False)
            (proj): Linear(in_features=1024, out_features=1024, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.143)
          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (mlp): SwiGLU(
            (w1): Linear(in_features=1024, out_features=2730, bias=True)
            (w2): Linear(in_features=1024, out_features=2730, bias=True)
            (act): SiLU()
            (ffn_ln): LayerNorm((2730,), eps=1e-06, elementwise_affine=True)
            (w3): Linear(in_features=2730, out_features=1024, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (12): Block(
          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q_proj): Linear(in_features=1024, out_features=1024, bias=False)
            (k_proj): Linear(in_features=1024, out_features=1024, bias=False)
            (v_proj): Linear(in_features=1024, out_features=1024, bias=False)
            (rope): VisionRotaryEmbeddingFast()
            (proj): Linear(in_features=1024, out_features=1024, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.157)
          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (mlp): SwiGLU(
            (w1): Linear(in_features=1024, out_features=2730, bias=True)
            (w2): Linear(in_features=1024, out_features=2730, bias=True)
            (act): SiLU()
            (ffn_ln): LayerNorm((2730,), eps=1e-06, elementwise_affine=True)
            (w3): Linear(in_features=2730, out_features=1024, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (13): Block(
          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q_proj): Linear(in_features=1024, out_features=1024, bias=False)
            (k_proj): Linear(in_features=1024, out_features=1024, bias=False)
            (v_proj): Linear(in_features=1024, out_features=1024, bias=False)
            (rope): VisionRotaryEmbeddingFast()
            (proj): Linear(in_features=1024, out_features=1024, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.170)
          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (mlp): SwiGLU(
            (w1): Linear(in_features=1024, out_features=2730, bias=True)
            (w2): Linear(in_features=1024, out_features=2730, bias=True)
            (act): SiLU()
            (ffn_ln): LayerNorm((2730,), eps=1e-06, elementwise_affine=True)
            (w3): Linear(in_features=2730, out_features=1024, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (14): Block(
          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q_proj): Linear(in_features=1024, out_features=1024, bias=False)
            (k_proj): Linear(in_features=1024, out_features=1024, bias=False)
            (v_proj): Linear(in_features=1024, out_features=1024, bias=False)
            (proj): Linear(in_features=1024, out_features=1024, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.183)
          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (mlp): SwiGLU(
            (w1): Linear(in_features=1024, out_features=2730, bias=True)
            (w2): Linear(in_features=1024, out_features=2730, bias=True)
            (act): SiLU()
            (ffn_ln): LayerNorm((2730,), eps=1e-06, elementwise_affine=True)
            (w3): Linear(in_features=2730, out_features=1024, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (15): Block(
          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q_proj): Linear(in_features=1024, out_features=1024, bias=False)
            (k_proj): Linear(in_features=1024, out_features=1024, bias=False)
            (v_proj): Linear(in_features=1024, out_features=1024, bias=False)
            (rope): VisionRotaryEmbeddingFast()
            (proj): Linear(in_features=1024, out_features=1024, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.196)
          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (mlp): SwiGLU(
            (w1): Linear(in_features=1024, out_features=2730, bias=True)
            (w2): Linear(in_features=1024, out_features=2730, bias=True)
            (act): SiLU()
            (ffn_ln): LayerNorm((2730,), eps=1e-06, elementwise_affine=True)
            (w3): Linear(in_features=2730, out_features=1024, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (16): Block(
          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q_proj): Linear(in_features=1024, out_features=1024, bias=False)
            (k_proj): Linear(in_features=1024, out_features=1024, bias=False)
            (v_proj): Linear(in_features=1024, out_features=1024, bias=False)
            (rope): VisionRotaryEmbeddingFast()
            (proj): Linear(in_features=1024, out_features=1024, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.209)
          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (mlp): SwiGLU(
            (w1): Linear(in_features=1024, out_features=2730, bias=True)
            (w2): Linear(in_features=1024, out_features=2730, bias=True)
            (act): SiLU()
            (ffn_ln): LayerNorm((2730,), eps=1e-06, elementwise_affine=True)
            (w3): Linear(in_features=2730, out_features=1024, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (17): Block(
          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q_proj): Linear(in_features=1024, out_features=1024, bias=False)
            (k_proj): Linear(in_features=1024, out_features=1024, bias=False)
            (v_proj): Linear(in_features=1024, out_features=1024, bias=False)
            (proj): Linear(in_features=1024, out_features=1024, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.222)
          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (mlp): SwiGLU(
            (w1): Linear(in_features=1024, out_features=2730, bias=True)
            (w2): Linear(in_features=1024, out_features=2730, bias=True)
            (act): SiLU()
            (ffn_ln): LayerNorm((2730,), eps=1e-06, elementwise_affine=True)
            (w3): Linear(in_features=2730, out_features=1024, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (18): Block(
          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q_proj): Linear(in_features=1024, out_features=1024, bias=False)
            (k_proj): Linear(in_features=1024, out_features=1024, bias=False)
            (v_proj): Linear(in_features=1024, out_features=1024, bias=False)
            (rope): VisionRotaryEmbeddingFast()
            (proj): Linear(in_features=1024, out_features=1024, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.235)
          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (mlp): SwiGLU(
            (w1): Linear(in_features=1024, out_features=2730, bias=True)
            (w2): Linear(in_features=1024, out_features=2730, bias=True)
            (act): SiLU()
            (ffn_ln): LayerNorm((2730,), eps=1e-06, elementwise_affine=True)
            (w3): Linear(in_features=2730, out_features=1024, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (19): Block(
          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q_proj): Linear(in_features=1024, out_features=1024, bias=False)
            (k_proj): Linear(in_features=1024, out_features=1024, bias=False)
            (v_proj): Linear(in_features=1024, out_features=1024, bias=False)
            (rope): VisionRotaryEmbeddingFast()
            (proj): Linear(in_features=1024, out_features=1024, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.248)
          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (mlp): SwiGLU(
            (w1): Linear(in_features=1024, out_features=2730, bias=True)
            (w2): Linear(in_features=1024, out_features=2730, bias=True)
            (act): SiLU()
            (ffn_ln): LayerNorm((2730,), eps=1e-06, elementwise_affine=True)
            (w3): Linear(in_features=2730, out_features=1024, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (20): Block(
          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q_proj): Linear(in_features=1024, out_features=1024, bias=False)
            (k_proj): Linear(in_features=1024, out_features=1024, bias=False)
            (v_proj): Linear(in_features=1024, out_features=1024, bias=False)
            (proj): Linear(in_features=1024, out_features=1024, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.261)
          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (mlp): SwiGLU(
            (w1): Linear(in_features=1024, out_features=2730, bias=True)
            (w2): Linear(in_features=1024, out_features=2730, bias=True)
            (act): SiLU()
            (ffn_ln): LayerNorm((2730,), eps=1e-06, elementwise_affine=True)
            (w3): Linear(in_features=2730, out_features=1024, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (21): Block(
          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q_proj): Linear(in_features=1024, out_features=1024, bias=False)
            (k_proj): Linear(in_features=1024, out_features=1024, bias=False)
            (v_proj): Linear(in_features=1024, out_features=1024, bias=False)
            (rope): VisionRotaryEmbeddingFast()
            (proj): Linear(in_features=1024, out_features=1024, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.274)
          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (mlp): SwiGLU(
            (w1): Linear(in_features=1024, out_features=2730, bias=True)
            (w2): Linear(in_features=1024, out_features=2730, bias=True)
            (act): SiLU()
            (ffn_ln): LayerNorm((2730,), eps=1e-06, elementwise_affine=True)
            (w3): Linear(in_features=2730, out_features=1024, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (22): Block(
          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q_proj): Linear(in_features=1024, out_features=1024, bias=False)
            (k_proj): Linear(in_features=1024, out_features=1024, bias=False)
            (v_proj): Linear(in_features=1024, out_features=1024, bias=False)
            (rope): VisionRotaryEmbeddingFast()
            (proj): Linear(in_features=1024, out_features=1024, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.287)
          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (mlp): SwiGLU(
            (w1): Linear(in_features=1024, out_features=2730, bias=True)
            (w2): Linear(in_features=1024, out_features=2730, bias=True)
            (act): SiLU()
            (ffn_ln): LayerNorm((2730,), eps=1e-06, elementwise_affine=True)
            (w3): Linear(in_features=2730, out_features=1024, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (23): Block(
          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q_proj): Linear(in_features=1024, out_features=1024, bias=False)
            (k_proj): Linear(in_features=1024, out_features=1024, bias=False)
            (v_proj): Linear(in_features=1024, out_features=1024, bias=False)
            (proj): Linear(in_features=1024, out_features=1024, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.300)
          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (mlp): SwiGLU(
            (w1): Linear(in_features=1024, out_features=2730, bias=True)
            (w2): Linear(in_features=1024, out_features=2730, bias=True)
            (act): SiLU()
            (ffn_ln): LayerNorm((2730,), eps=1e-06, elementwise_affine=True)
            (w3): Linear(in_features=2730, out_features=1024, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (out_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    )
    (neck): SFP(
      (p2): Sequential(
        (0): Upsample(scale_factor=2.0, mode=nearest)
        (1): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (2): LayerNorm()
        (3): GELU()
        (4): Upsample(scale_factor=2.0, mode=nearest)
        (5): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (6): LayerNorm()
        (7): GELU()
        (8): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (9): LayerNorm()
        (10): GELU()
        (11): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (12): LayerNorm()
      )
      (p3): Sequential(
        (0): Upsample(scale_factor=2.0, mode=nearest)
        (1): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (2): LayerNorm()
        (3): GELU()
        (4): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (5): LayerNorm()
        (6): GELU()
        (7): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (8): LayerNorm()
      )
      (p4): Sequential(
        (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): LayerNorm()
        (2): GELU()
        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (4): LayerNorm()
      )
      (p5): Sequential(
        (0): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
        (1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (2): LayerNorm()
        (3): GELU()
        (4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (5): LayerNorm()
      )
      (p6): Sequential(
        (0): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (2): LayerNorm()
        (3): GELU()
        (4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (5): LayerNorm()
        (6): GELU()
        (7): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (8): LayerNorm()
      )
    )
    init_cfg={'type': 'Xavier', 'layer': 'Conv2d', 'distribution': 'uniform'}
    (query_head): CoDINOHead(
      (loss_cls): QualityFocalLoss()
      (loss_bbox): L1Loss()
      (loss_iou): GIoULoss()
      (activate): ReLU(inplace=True)
      (positional_encoding): SinePositionalEncoding(num_feats=128, temperature=20, normalize=True, scale=6.283185307179586, eps=1e-06)
      (transformer): CoDinoTransformer(
        (encoder): DetrTransformerEncoder(
          (layers): ModuleList(
            (0): BaseTransformerLayer(
              (attentions): ModuleList(
                (0): MultiScaleDeformableAttention(
                  (dropout): Dropout(p=0.0, inplace=False)
                  (sampling_offsets): Linear(in_features=256, out_features=320, bias=True)
                  (attention_weights): Linear(in_features=256, out_features=160, bias=True)
                  (value_proj): Linear(in_features=256, out_features=256, bias=True)
                  (output_proj): Linear(in_features=256, out_features=256, bias=True)
                )
              )
              (ffns): ModuleList(
                (0): FFN(
                  (activate): ReLU(inplace=True)
                  (layers): Sequential(
                    (0): Sequential(
                      (0): Linear(in_features=256, out_features=2048, bias=True)
                      (1): ReLU(inplace=True)
                      (2): Dropout(p=0.0, inplace=False)
                    )
                    (1): Linear(in_features=2048, out_features=256, bias=True)
                    (2): Dropout(p=0.0, inplace=False)
                  )
                  (dropout_layer): Identity()
                )
              )
              (norms): ModuleList(
                (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              )
            )
            (1): BaseTransformerLayer(
              (attentions): ModuleList(
                (0): MultiScaleDeformableAttention(
                  (dropout): Dropout(p=0.0, inplace=False)
                  (sampling_offsets): Linear(in_features=256, out_features=320, bias=True)
                  (attention_weights): Linear(in_features=256, out_features=160, bias=True)
                  (value_proj): Linear(in_features=256, out_features=256, bias=True)
                  (output_proj): Linear(in_features=256, out_features=256, bias=True)
                )
              )
              (ffns): ModuleList(
                (0): FFN(
                  (activate): ReLU(inplace=True)
                  (layers): Sequential(
                    (0): Sequential(
                      (0): Linear(in_features=256, out_features=2048, bias=True)
                      (1): ReLU(inplace=True)
                      (2): Dropout(p=0.0, inplace=False)
                    )
                    (1): Linear(in_features=2048, out_features=256, bias=True)
                    (2): Dropout(p=0.0, inplace=False)
                  )
                  (dropout_layer): Identity()
                )
              )
              (norms): ModuleList(
                (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              )
            )
            (2): BaseTransformerLayer(
              (attentions): ModuleList(
                (0): MultiScaleDeformableAttention(
                  (dropout): Dropout(p=0.0, inplace=False)
                  (sampling_offsets): Linear(in_features=256, out_features=320, bias=True)
                  (attention_weights): Linear(in_features=256, out_features=160, bias=True)
                  (value_proj): Linear(in_features=256, out_features=256, bias=True)
                  (output_proj): Linear(in_features=256, out_features=256, bias=True)
                )
              )
              (ffns): ModuleList(
                (0): FFN(
                  (activate): ReLU(inplace=True)
                  (layers): Sequential(
                    (0): Sequential(
                      (0): Linear(in_features=256, out_features=2048, bias=True)
                      (1): ReLU(inplace=True)
                      (2): Dropout(p=0.0, inplace=False)
                    )
                    (1): Linear(in_features=2048, out_features=256, bias=True)
                    (2): Dropout(p=0.0, inplace=False)
                  )
                  (dropout_layer): Identity()
                )
              )
              (norms): ModuleList(
                (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              )
            )
            (3): BaseTransformerLayer(
              (attentions): ModuleList(
                (0): MultiScaleDeformableAttention(
                  (dropout): Dropout(p=0.0, inplace=False)
                  (sampling_offsets): Linear(in_features=256, out_features=320, bias=True)
                  (attention_weights): Linear(in_features=256, out_features=160, bias=True)
                  (value_proj): Linear(in_features=256, out_features=256, bias=True)
                  (output_proj): Linear(in_features=256, out_features=256, bias=True)
                )
              )
              (ffns): ModuleList(
                (0): FFN(
                  (activate): ReLU(inplace=True)
                  (layers): Sequential(
                    (0): Sequential(
                      (0): Linear(in_features=256, out_features=2048, bias=True)
                      (1): ReLU(inplace=True)
                      (2): Dropout(p=0.0, inplace=False)
                    )
                    (1): Linear(in_features=2048, out_features=256, bias=True)
                    (2): Dropout(p=0.0, inplace=False)
                  )
                  (dropout_layer): Identity()
                )
              )
              (norms): ModuleList(
                (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              )
            )
            (4): BaseTransformerLayer(
              (attentions): ModuleList(
                (0): MultiScaleDeformableAttention(
                  (dropout): Dropout(p=0.0, inplace=False)
                  (sampling_offsets): Linear(in_features=256, out_features=320, bias=True)
                  (attention_weights): Linear(in_features=256, out_features=160, bias=True)
                  (value_proj): Linear(in_features=256, out_features=256, bias=True)
                  (output_proj): Linear(in_features=256, out_features=256, bias=True)
                )
              )
              (ffns): ModuleList(
                (0): FFN(
                  (activate): ReLU(inplace=True)
                  (layers): Sequential(
                    (0): Sequential(
                      (0): Linear(in_features=256, out_features=2048, bias=True)
                      (1): ReLU(inplace=True)
                      (2): Dropout(p=0.0, inplace=False)
                    )
                    (1): Linear(in_features=2048, out_features=256, bias=True)
                    (2): Dropout(p=0.0, inplace=False)
                  )
                  (dropout_layer): Identity()
                )
              )
              (norms): ModuleList(
                (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              )
            )
            (5): BaseTransformerLayer(
              (attentions): ModuleList(
                (0): MultiScaleDeformableAttention(
                  (dropout): Dropout(p=0.0, inplace=False)
                  (sampling_offsets): Linear(in_features=256, out_features=320, bias=True)
                  (attention_weights): Linear(in_features=256, out_features=160, bias=True)
                  (value_proj): Linear(in_features=256, out_features=256, bias=True)
                  (output_proj): Linear(in_features=256, out_features=256, bias=True)
                )
              )
              (ffns): ModuleList(
                (0): FFN(
                  (activate): ReLU(inplace=True)
                  (layers): Sequential(
                    (0): Sequential(
                      (0): Linear(in_features=256, out_features=2048, bias=True)
                      (1): ReLU(inplace=True)
                      (2): Dropout(p=0.0, inplace=False)
                    )
                    (1): Linear(in_features=2048, out_features=256, bias=True)
                    (2): Dropout(p=0.0, inplace=False)
                  )
                  (dropout_layer): Identity()
                )
              )
              (norms): ModuleList(
                (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              )
            )
          )
        )
        (decoder): DinoTransformerDecoder(
          (layers): ModuleList(
            (0): DetrTransformerDecoderLayer(
              (attentions): ModuleList(
                (0): MultiheadAttention(
                  (attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                  )
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (dropout_layer): Dropout(p=0.0, inplace=False)
                )
                (1): MultiScaleDeformableAttention(
                  (dropout): Dropout(p=0.0, inplace=False)
                  (sampling_offsets): Linear(in_features=256, out_features=320, bias=True)
                  (attention_weights): Linear(in_features=256, out_features=160, bias=True)
                  (value_proj): Linear(in_features=256, out_features=256, bias=True)
                  (output_proj): Linear(in_features=256, out_features=256, bias=True)
                )
              )
              (ffns): ModuleList(
                (0): FFN(
                  (activate): ReLU(inplace=True)
                  (layers): Sequential(
                    (0): Sequential(
                      (0): Linear(in_features=256, out_features=2048, bias=True)
                      (1): ReLU(inplace=True)
                      (2): Dropout(p=0.0, inplace=False)
                    )
                    (1): Linear(in_features=2048, out_features=256, bias=True)
                    (2): Dropout(p=0.0, inplace=False)
                  )
                  (dropout_layer): Identity()
                )
              )
              (norms): ModuleList(
                (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              )
            )
            (1): DetrTransformerDecoderLayer(
              (attentions): ModuleList(
                (0): MultiheadAttention(
                  (attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                  )
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (dropout_layer): Dropout(p=0.0, inplace=False)
                )
                (1): MultiScaleDeformableAttention(
                  (dropout): Dropout(p=0.0, inplace=False)
                  (sampling_offsets): Linear(in_features=256, out_features=320, bias=True)
                  (attention_weights): Linear(in_features=256, out_features=160, bias=True)
                  (value_proj): Linear(in_features=256, out_features=256, bias=True)
                  (output_proj): Linear(in_features=256, out_features=256, bias=True)
                )
              )
              (ffns): ModuleList(
                (0): FFN(
                  (activate): ReLU(inplace=True)
                  (layers): Sequential(
                    (0): Sequential(
                      (0): Linear(in_features=256, out_features=2048, bias=True)
                      (1): ReLU(inplace=True)
                      (2): Dropout(p=0.0, inplace=False)
                    )
                    (1): Linear(in_features=2048, out_features=256, bias=True)
                    (2): Dropout(p=0.0, inplace=False)
                  )
                  (dropout_layer): Identity()
                )
              )
              (norms): ModuleList(
                (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              )
            )
            (2): DetrTransformerDecoderLayer(
              (attentions): ModuleList(
                (0): MultiheadAttention(
                  (attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                  )
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (dropout_layer): Dropout(p=0.0, inplace=False)
                )
                (1): MultiScaleDeformableAttention(
                  (dropout): Dropout(p=0.0, inplace=False)
                  (sampling_offsets): Linear(in_features=256, out_features=320, bias=True)
                  (attention_weights): Linear(in_features=256, out_features=160, bias=True)
                  (value_proj): Linear(in_features=256, out_features=256, bias=True)
                  (output_proj): Linear(in_features=256, out_features=256, bias=True)
                )
              )
              (ffns): ModuleList(
                (0): FFN(
                  (activate): ReLU(inplace=True)
                  (layers): Sequential(
                    (0): Sequential(
                      (0): Linear(in_features=256, out_features=2048, bias=True)
                      (1): ReLU(inplace=True)
                      (2): Dropout(p=0.0, inplace=False)
                    )
                    (1): Linear(in_features=2048, out_features=256, bias=True)
                    (2): Dropout(p=0.0, inplace=False)
                  )
                  (dropout_layer): Identity()
                )
              )
              (norms): ModuleList(
                (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              )
            )
            (3): DetrTransformerDecoderLayer(
              (attentions): ModuleList(
                (0): MultiheadAttention(
                  (attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                  )
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (dropout_layer): Dropout(p=0.0, inplace=False)
                )
                (1): MultiScaleDeformableAttention(
                  (dropout): Dropout(p=0.0, inplace=False)
                  (sampling_offsets): Linear(in_features=256, out_features=320, bias=True)
                  (attention_weights): Linear(in_features=256, out_features=160, bias=True)
                  (value_proj): Linear(in_features=256, out_features=256, bias=True)
                  (output_proj): Linear(in_features=256, out_features=256, bias=True)
                )
              )
              (ffns): ModuleList(
                (0): FFN(
                  (activate): ReLU(inplace=True)
                  (layers): Sequential(
                    (0): Sequential(
                      (0): Linear(in_features=256, out_features=2048, bias=True)
                      (1): ReLU(inplace=True)
                      (2): Dropout(p=0.0, inplace=False)
                    )
                    (1): Linear(in_features=2048, out_features=256, bias=True)
                    (2): Dropout(p=0.0, inplace=False)
                  )
                  (dropout_layer): Identity()
                )
              )
              (norms): ModuleList(
                (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              )
            )
            (4): DetrTransformerDecoderLayer(
              (attentions): ModuleList(
                (0): MultiheadAttention(
                  (attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                  )
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (dropout_layer): Dropout(p=0.0, inplace=False)
                )
                (1): MultiScaleDeformableAttention(
                  (dropout): Dropout(p=0.0, inplace=False)
                  (sampling_offsets): Linear(in_features=256, out_features=320, bias=True)
                  (attention_weights): Linear(in_features=256, out_features=160, bias=True)
                  (value_proj): Linear(in_features=256, out_features=256, bias=True)
                  (output_proj): Linear(in_features=256, out_features=256, bias=True)
                )
              )
              (ffns): ModuleList(
                (0): FFN(
                  (activate): ReLU(inplace=True)
                  (layers): Sequential(
                    (0): Sequential(
                      (0): Linear(in_features=256, out_features=2048, bias=True)
                      (1): ReLU(inplace=True)
                      (2): Dropout(p=0.0, inplace=False)
                    )
                    (1): Linear(in_features=2048, out_features=256, bias=True)
                    (2): Dropout(p=0.0, inplace=False)
                  )
                  (dropout_layer): Identity()
                )
              )
              (norms): ModuleList(
                (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              )
            )
            (5): DetrTransformerDecoderLayer(
              (attentions): ModuleList(
                (0): MultiheadAttention(
                  (attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                  )
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (dropout_layer): Dropout(p=0.0, inplace=False)
                )
                (1): MultiScaleDeformableAttention(
                  (dropout): Dropout(p=0.0, inplace=False)
                  (sampling_offsets): Linear(in_features=256, out_features=320, bias=True)
                  (attention_weights): Linear(in_features=256, out_features=160, bias=True)
                  (value_proj): Linear(in_features=256, out_features=256, bias=True)
                  (output_proj): Linear(in_features=256, out_features=256, bias=True)
                )
              )
              (ffns): ModuleList(
                (0): FFN(
                  (activate): ReLU(inplace=True)
                  (layers): Sequential(
                    (0): Sequential(
                      (0): Linear(in_features=256, out_features=2048, bias=True)
                      (1): ReLU(inplace=True)
                      (2): Dropout(p=0.0, inplace=False)
                    )
                    (1): Linear(in_features=2048, out_features=256, bias=True)
                    (2): Dropout(p=0.0, inplace=False)
                  )
                  (dropout_layer): Identity()
                )
              )
              (norms): ModuleList(
                (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              )
            )
          )
          (ref_point_head): Sequential(
            (0): Linear(in_features=512, out_features=256, bias=True)
            (1): ReLU()
            (2): Linear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (enc_output): Linear(in_features=256, out_features=256, bias=True)
        (enc_output_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (query_embed): Embedding(900, 256)
        (aux_pos_trans): ModuleList(
          (0): Linear(in_features=512, out_features=256, bias=True)
          (1): Linear(in_features=512, out_features=256, bias=True)
        )
        (aux_pos_trans_norm): ModuleList(
          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (pos_feats_trans): ModuleList()
        (pos_feats_norm): ModuleList()
      )
      (downsample): Sequential(
        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (cls_branches): ModuleList(
        (0): Linear(in_features=256, out_features=1, bias=True)
        (1): Linear(in_features=256, out_features=1, bias=True)
        (2): Linear(in_features=256, out_features=1, bias=True)
        (3): Linear(in_features=256, out_features=1, bias=True)
        (4): Linear(in_features=256, out_features=1, bias=True)
        (5): Linear(in_features=256, out_features=1, bias=True)
        (6): Linear(in_features=256, out_features=1, bias=True)
      )
      (reg_branches): ModuleList(
        (0): Sequential(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): ReLU()
          (2): Linear(in_features=256, out_features=256, bias=True)
          (3): ReLU()
          (4): Linear(in_features=256, out_features=4, bias=True)
        )
        (1): Sequential(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): ReLU()
          (2): Linear(in_features=256, out_features=256, bias=True)
          (3): ReLU()
          (4): Linear(in_features=256, out_features=4, bias=True)
        )
        (2): Sequential(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): ReLU()
          (2): Linear(in_features=256, out_features=256, bias=True)
          (3): ReLU()
          (4): Linear(in_features=256, out_features=4, bias=True)
        )
        (3): Sequential(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): ReLU()
          (2): Linear(in_features=256, out_features=256, bias=True)
          (3): ReLU()
          (4): Linear(in_features=256, out_features=4, bias=True)
        )
        (4): Sequential(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): ReLU()
          (2): Linear(in_features=256, out_features=256, bias=True)
          (3): ReLU()
          (4): Linear(in_features=256, out_features=4, bias=True)
        )
        (5): Sequential(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): ReLU()
          (2): Linear(in_features=256, out_features=256, bias=True)
          (3): ReLU()
          (4): Linear(in_features=256, out_features=4, bias=True)
        )
        (6): Sequential(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): ReLU()
          (2): Linear(in_features=256, out_features=256, bias=True)
          (3): ReLU()
          (4): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (query_embedding): None
      (label_embedding): Embedding(1, 256)
    )
    (rpn_head): RPNHead(
      (loss_cls): CrossEntropyLoss(avg_non_ignore=False)
      (loss_bbox): L1Loss()
      (rpn_conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (rpn_cls): Conv2d(256, 9, kernel_size=(1, 1), stride=(1, 1))
      (rpn_reg): Conv2d(256, 36, kernel_size=(1, 1), stride=(1, 1))
    )
    init_cfg={'type': 'Normal', 'layer': 'Conv2d', 'std': 0.01}
    (roi_head): ModuleList(
      (0): CoStandardRoIHead(
        (bbox_roi_extractor): SingleRoIExtractor(
          (roi_layers): ModuleList(
            (0): RoIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, pool_mode=avg, aligned=True, use_torchvision=False)
            (1): RoIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, pool_mode=avg, aligned=True, use_torchvision=False)
            (2): RoIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, pool_mode=avg, aligned=True, use_torchvision=False)
            (3): RoIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, pool_mode=avg, aligned=True, use_torchvision=False)
            (4): RoIAlign(output_size=(7, 7), spatial_scale=0.015625, sampling_ratio=0, pool_mode=avg, aligned=True, use_torchvision=False)
          )
        )
        (bbox_head): Shared2FCBBoxHead(
          (loss_cls): CrossEntropyLoss(avg_non_ignore=False)
          (loss_bbox): GIoULoss()
          (fc_cls): Linear(in_features=1024, out_features=2, bias=True)
          (fc_reg): Linear(in_features=1024, out_features=4, bias=True)
          (shared_convs): ModuleList()
          (shared_fcs): ModuleList(
            (0): Linear(in_features=12544, out_features=1024, bias=True)
            (1): Linear(in_features=1024, out_features=1024, bias=True)
          )
          (cls_convs): ModuleList()
          (cls_fcs): ModuleList()
          (reg_convs): ModuleList()
          (reg_fcs): ModuleList()
          (relu): ReLU(inplace=True)
        )
        init_cfg=[{'type': 'Normal', 'std': 0.01, 'override': {'name': 'fc_cls'}}, {'type': 'Normal', 'std': 0.001, 'override': {'name': 'fc_reg'}}, {'type': 'Xavier', 'distribution': 'uniform', 'override': [{'name': 'shared_fcs'}, {'name': 'cls_fcs'}, {'name': 'reg_fcs'}]}]
      )
    )
    (bbox_head): ModuleList(
      (0): CoATSSHead(
        (loss_cls): FocalLoss()
        (loss_bbox): GIoULoss()
        (relu): ReLU(inplace=True)
        (cls_convs): ModuleList(
          (0): ConvModule(
            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (gn): GroupNorm(32, 256, eps=1e-05, affine=True)
            (activate): ReLU(inplace=True)
          )
        )
        (reg_convs): ModuleList(
          (0): ConvModule(
            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (gn): GroupNorm(32, 256, eps=1e-05, affine=True)
            (activate): ReLU(inplace=True)
          )
        )
        (atss_cls): Conv2d(256, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (atss_reg): Conv2d(256, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (atss_centerness): Conv2d(256, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (scales): ModuleList(
          (0): Scale()
          (1): Scale()
          (2): Scale()
          (3): Scale()
          (4): Scale()
          (5): Scale()
        )
        (loss_centerness): CrossEntropyLoss(avg_non_ignore=False)
      )
      init_cfg={'type': 'Normal', 'layer': 'Conv2d', 'std': 0.01, 'override': {'type': 'Normal', 'name': 'atss_cls', 'std': 0.01, 'bias_prob': 0.01}}
    )
  )
)
